{"cells":[{"cell_type":"markdown","metadata":{"id":"D_6-jdBOXowG"},"source":["# 演習の方針\n","\n","1. **ベースラインモデル評価**  \n","   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n","\n","2. **文字起こしデータの活用**  \n","   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n","\n","3. **チャンク化の導入**  \n","   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n","\n","4. **Rerankの適用**  \n","   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n","\n","5. **応用改善手法**  \n","   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"]},{"cell_type":"markdown","metadata":{"id":"PPI1pj4mFavt"},"source":["## 扱う質問\n","\n","「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n","\n","- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n","- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n","- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n","\n","- 模範解答:\n","```\n","「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\n","```\n","\n","## 扱うモデル\n","\n","Meta-Llama-3-8B-Instruct（2024年4月リリース）を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n","\n","- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n","- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"]},{"cell_type":"markdown","metadata":{"id":"eCGlMRPXSuim"},"source":["### 演習環境の準備"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vM50WAI7GXwC","collapsed":true,"outputId":"535f1d31-cf68-4dc1-ffa2-4f5bec1eb299","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595104931,"user_tz":-540,"elapsed":6921,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: google-colab-selenium in /usr/local/lib/python3.11/dist-packages (1.0.14)\n","Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium) (4.32.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.4.0)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.30.0)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.12.2)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2025.4.26)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.13.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (25.3.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium) (1.2.0)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.16.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"]}],"source":["!pip install --upgrade transformers\n","!pip install google-colab-selenium\n","!pip install bitsandbytes"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_vs-Ri1Tslqw","outputId":"1e3421cc-2350-483b-c458-b63868c88c57","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595110385,"user_tz":-540,"elapsed":1136,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'lecture-ai-engineering'...\n","remote: Enumerating objects: 52, done.\u001b[K\n","remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 52 (from 1)\u001b[K\n","Receiving objects: 100% (52/52), 83.21 KiB | 20.80 MiB/s, done.\n","Resolving deltas: 100% (9/9), done.\n"]}],"source":["# 演習用のコンテンツを取得\n","!git clone https://github.com/matsuolab/lecture-ai-engineering.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zXo_kFASXlvp","outputId":"bb9b6d0d-629e-4ee5-b1b8-c2ccf0dec626","colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["b2347474ce7b4392af8185398e0c2f59","5ed556d2e9e546cb871fc191f87ef830","c5bd1ed2012048a09d343c67ad36719c","8fab02b85ba44107a25894c1f7f6ab83","1752a457e376443e87ac407cf143e154","4d9d19b3a5fc4e7dbf4a696509bb0403","0836aed21a48439a8080e315889f9102","243190d4f9f7424c8917d94e3daaa1bc","96c82411afb64a72ae3a33898bfa2f42","6fda146929eb427f8bcf457d0a6f1f20","2a08d63f64e948239faff9c585e6b6ce","329afa3acf454194a21343fc3d46e8c6","c6238b45df974199a2a741c4c0c545cf","5f89b4de5da74340963db0adf1b6c30f","9139b66a4e21410cbd0705719a8a22e4","87b873e5060343ec8f70cc16ff3b5492","db706ef327ba442992dfab9a4dc30daa","e022946c478449c7baef8c9c9ec9919a","4c2c0570c32b433c8219acad4e63c894","96a55fabb715454ab121b7d81b255e71"]},"executionInfo":{"status":"ok","timestamp":1746595113342,"user_tz":-540,"elapsed":285,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2347474ce7b4392af8185398e0c2f59"}},"metadata":{}}],"source":["# HuggingFace Login\n","from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dZ_NUIftXwLc","executionInfo":{"status":"ok","timestamp":1746595130761,"user_tz":-540,"elapsed":1604,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[],"source":["# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7eTgV8XBPA90","executionInfo":{"status":"ok","timestamp":1746595132969,"user_tz":-540,"elapsed":7,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[],"source":["import random\n","random.seed(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6tV9mO8oXoaM","outputId":"4669b9be-0c25-431e-8aa2-f0725c9dcbe6","colab":{"base_uri":"https://localhost:8080/","height":511,"referenced_widgets":["7b3d5280d3da4f62a690cacddbdbefe3","73278654927e48cc84eb544dce4471dd","5b307ac2315c461fb52e2cead04d9e6f","3ed5258da3cd4741adb99a0e36349fcd","a3a5b151232142d7b1b3dcaaa36f8943","7ca7b1039ffb4cd6b735f0e29139778b","812674ea6ae64c8a972c2c379bef9d33","85ac1c2bbea8496680c8cde55e23a647","bf01474b2dc040b99cd409f285b6531b","0983feaa5f0740ee8a19cf92af540cc2","40d72d7b76484ee287bfeeb3f0a703f8","b42b399ccbec4d1baef0e346bc941796","2a65b9c35ddc4c72ab31f5dd614535a3","ae947a16873940a9bb041bffdf40d67d","68b98be0df0f4de0870cbd7bca004c08","27384b495e044cb7948602a3812f8dee","194b20e861a9442ab80f05f0d898dca3","29b31a996d2b4c1484c9145cf19e1c77","0aee21e50e02490598e86bc499c1f9d8","7530e8bd4ca3437c92afb201369ad6e8","c3d2702862004bff8e2fd708839f874f","6881736cf1ff4a9e8a68427b5d5949f1","05b9d16280364170aaa267470ac6c04f","9e41c161cfb148e09692637a00d3f1e1","e72bcddf80ca4af1a9951b4651f2fe40","3e52da1d701c4fd5bd2caffebe97c5ba","4494c762b50f4223938b95e71327454c","11784e388ab244c69e621b88cb0f7212","bbe203b392fd40f8a4a66ee64f4fcd8a","47002b4679a9490bbe4f828d6aaa9d34","5efeca998801446493598db5300e3ac4","4f7c2ac6cc854cd0906d775736b0bc2b","0b2b45a3ae7f49628eb51813b0ffe35b","b21f7f5603864f04a2e7d6dc3491d151","37d8be17b46b40968c2e368768f63324","6f70f26132814dd9a0109d120d7cd96f","f3d25b31cd64478391d979be1f7c4ecf","febec18b49a3400a9aba921135a503d8","9f498797c8e94014952e0dbfaa8c3307","255ae5a0af3e40488f119ef7851548e9","072180ffb39e4376aa2795feef569057","0b032aaa06844d84a25c762bacec5c6f","6f96b04fb9704d99bf6d6a746c12a842","ed1f1151372e4494bdb3ff00972dd99b","5f42e32817a940a0a72ab7243530f8ef","6fab2ac024124038b49b7e0753466f75","5fb3093e063540c1bb2f897f8e427a06","c89e03b65a54484ead8fce7126fc8023","65f68584dee94214bded0c1b922e5a85","90bd5954a445439db9f6f193534d8831","46152ffdb9a242a49b8fe9462851093e","09095a3665bd40a3b0169aa805ae65ff","6654dcbde3f046ff8cede5c99e53299a","0fde6d2192864d6eae0c6a3d2bb93345","ae0fbf024d2349238f6ebb9d316e5c65","1869efabe4c1417f8906380aafedec09","a75ad1579863429c8fcb81570eed169d","0022f39d78e548f9ac96d93e916b74d2","ac6763bee19f4f28a56559fbcf5f4715","82c3046b688a4d7f9688ceb62762c440","086898708b1e4c4e95554d7496169320","fcbb422ad37746109148a9e339a031a1","420ee7f0805841c09ddcb01e5e117087","529afedf045b4127817836c6ed51caa7","de0301fb7d644b32ab884eee30672ee6","41b04c78fc5644d3ba2f2d817823517a","d8b5ad882e9e4185b948184542839373","3d1e128f69e74ee8891704c77f51a0eb","ea3ad22160dc4ea9befbcc4ad3f752d3","f7e6cbe15a0f4725a89bb808f89d622a","eb5f419438344bb69ef8324f212c0c8a","327d4d17454b4e6280afc19143d237a9","393f5738216d44558be0490d34290551","242fa28d95004afe8b7a866f983b3091","a1bb5f8db4df4db285c253c610640215","777914645038483390d10017afda93fe","0ccbfe8739b64af6be6590169059f4a7","0667bde683824da7baacff3e8cecb609","23527478133c43a09dad9f82f831d043","7b54e8dd4f6f41d1992b81956e976dbf","caf2e60a294a4facb1d2460fbc7ae97e","e4325e1dc457464e8bee681c0f0edae6","06b2f559b73b46b9881dbc4455712fae","381bb40575ef4c9f9c7407ba24f011a9","7811bde2717444739e179ee9f798c850","50074d0252974c24a1d786a9ab9186ca","a3533b1a0c0044fbabdb646165565f05","84e57391650942d687ee9656fbdf9f4f","c9f06078d2a5451c908fd7e19507b623","252b98f97fde4430b573d734555dc27a","1ff79161fb58432f9434518ffc2e4e2f","48c1a26f723948cf888afcc0271745a3","146ac48e57974d31ab95a2b5526dd486","4cbe3cb201e943018358d5e0c88116cf","0c76932a68dd4945b20698c14afea86d","eb1c7e4b822e4c9a99dbf29686509cea","e06da87dff904e4a85801b868b57ddf1","103e755ed5c541c8aeef572899fa7e49","aac9a79545ac4fb5b0fa82619b0dc099","725039c3cd014afbb56f51d2874320d5","9fa21547710946e7b52527f18a8946ea","d1d087e0143946e99761212b44288e23","b6145ef88515477089a509be5365e415","2e2cf850bca746b3a8605d7456849524","79e326aa190b46c98e533facfe722b54","d3373f2f05324783be993485e5b6e4ca","338b310d399c441795e634957a3a86d9","bd1f0a7866cf416bb2012120e8192d12","bb083e7843764e89bc3befd650dd7164","481158b7d84a4bfd9501016e8ac4c43d","52c602196a764485a30154b467639d1a","14f05935a2fb4d88bac8dc214ade79a5","288c5229e59c4ff0be28c04cf9249f41","cb0a509423264d26983612af2d14de11","98e86a8e219949ab99243a8966c83fcc","d31a7ad8c68d4696bcefbe9ca4f983b5","8caa3cec07744230b086052f8567eb9a","842eaae521174e828b391fd490395bf7","df98f42be30447e2bfc9913403d60b87","6b4ff7f22d7e4db789b471a4008775d9","4e5ef4f88cfe4c5f88252288d43e6a2d","3e961e0172894d5aacfd68d169459178","0d270e8befeb43c2956bb48003e35974","2976ac42679c481ca45abfa8e9dd2235","b46654162d084c3e8de4bee41512cb33","8c8cc35b95214f5ab7659e656d9b6134","4da809853b1640439e941358a0f38d8e","0e283e09688e4febb699f4789be996b7","941aa19f00454faf9c0673532fed042f","ef0eae54863f4af5b978e2b40bc44886","7742b64b7f064344bdcabbb73c4bc361","cedd1ba46d3949eda270cfc3c9b679b0"]},"executionInfo":{"status":"ok","timestamp":1746595264515,"user_tz":-540,"elapsed":130041,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3d5280d3da4f62a690cacddbdbefe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42b399ccbec4d1baef0e346bc941796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b9d16280364170aaa267470ac6c04f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b21f7f5603864f04a2e7d6dc3491d151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f42e32817a940a0a72ab7243530f8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1869efabe4c1417f8906380aafedec09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b5ad882e9e4185b948184542839373"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0667bde683824da7baacff3e8cecb609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f06078d2a5451c908fd7e19507b623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725039c3cd014afbb56f51d2874320d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c602196a764485a30154b467639d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e961e0172894d5aacfd68d169459178"}},"metadata":{}}],"source":["# モデル(Llama3)の読み込み\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            device_map=\"auto\",\n","            quantization_config=bnb_config,\n","            torch_dtype=torch.bfloat16,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"piTdVxTfGcc_"},"source":["# 1. ベースラインモデル評価\n","**まずはベースモデルがどの程度知識を持っているか確かめる**"]},{"cell_type":"code","source":["def generate_output(query, system_prompt=None):\n","  if system_prompt is None:\n","    messages = [\n","        {\"role\": \"user\", \"content\": query},\n","    ]\n","  else:\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": query},\n","    ]\n","  input_ids = tokenizer.apply_chat_template(\n","      messages,\n","      add_generation_prompt=True,\n","      return_tensors=\"pt\"\n","  ).to(model.device)\n","\n","  terminators = [\n","      tokenizer.eos_token_id,\n","      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","  ]\n","\n","  outputs = model.generate(\n","      input_ids,\n","      max_new_tokens=256,\n","      eos_token_id=terminators,\n","      do_sample=False,\n","      # temperature=0.6, # If do_sample=True\n","      # top_p=0.9,  # If do_sample=True\n","  )\n","\n","  response = outputs[0][input_ids.shape[-1]:]\n","  return tokenizer.decode(response, skip_special_tokens=True)"],"metadata":{"id":"OiBbJ9l54rAL","executionInfo":{"status":"ok","timestamp":1746595307336,"user_tz":-540,"elapsed":46,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["- 数値的な評価も見てみます。RagasにはAnswer Accuracyという評価指標があります。今回はこちらを参考に実装した評価関数を利用して測っていきます。\n","\n","今回はLlama3では性能が不安定だったので、OpenAIのgpt-4oで評価していきます。従って、scoreの実行はopenAI APIキーを所持している関心がある方のみで良いです。"],"metadata":{"id":"fG9zI6_lAYsQ"}},{"cell_type":"code","source":["system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。\"\n","question =  \"LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFstm9BpiG2f","executionInfo":{"status":"ok","timestamp":1746595323093,"user_tz":-540,"elapsed":12768,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"3265d504-a46d-4332-aa20-36026d0fefca"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Sbjp_KQjfRm","executionInfo":{"status":"ok","timestamp":1746595327813,"user_tz":-540,"elapsed":9,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"7707b211-1a7e-40b7-e524-f5d8b427ea47"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、Chinchillaという名前のモデルアーキテクチャーに基づくスケーリング法です。Chinchillaは、Googleの研究者によって提案されたLLMのアーキテクチャーで、モデルサイズの増加に対応するために設計されたスケーリング法です。\n","\n","Chinchilla scaling lawsは、LLMのパラメーターのスケーリングを通じて、モデルサイズの増加に対応するために使用されます。具体的には、Chinchilla scaling lawsは、LLMのパラメーターのスケーリングを、モデルサイズの増加に対応するために使用されます。LLMのパラメーターのスケーリングには、以下のような方法が含まれます。\n","\n","1. パラメーターのスケーリング：LLMのパラメーターのスケーリングを、モデルサイズの増加に対応するために使用します。\n","2. モデルのスケーリング：LLMのモデルサイズを、モデルサイズの増加に対応するために使用します。\n","3. パラメーターの調整\n"]}]},{"cell_type":"code","source":["!pip install -U openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fncw05_I_IVl","outputId":"f31dc815-a4bf-4a54-8be7-25adf8d3dc26","collapsed":true,"executionInfo":{"status":"ok","timestamp":1746595333380,"user_tz":-540,"elapsed":3420,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n","Collecting openai\n","  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n","Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.0/662.0 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.76.2\n","    Uninstalling openai-1.76.2:\n","      Successfully uninstalled openai-1.76.2\n","Successfully installed openai-1.77.0\n"]}]},{"cell_type":"code","source":["# # @title 評価実装\n","# gold_answer = \"「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\"\n","\n","# from openai import OpenAI\n","# from google.colab import userdata\n","# client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"), max_retries=5, timeout=60)\n","\n","# def openai_generator(query):\n","\n","#         messages = [\n","#                     {\n","#                         \"role\": \"user\",\n","#                         \"content\": query\n","#                     }\n","#                 ]\n","\n","#         response = client.chat.completions.create(\n","#             model=\"gpt-4o-mini\",\n","#             messages=messages\n","#         )\n","#         return response.choices[0].message.content\n","\n","# def evaluate_answer_accuracy(query, response, reference):\n","\n","#     template_accuracy1 = (\n","#           \"Instruction: You are a world class state of the art assistant for rating \"\n","#           \"a User Answer given a Question. The Question is completely answered by the Reference Answer.\\n\"\n","#           \"Say 4, if User Answer is full contained and equivalent to Reference Answer\"\n","#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n","#           \"Say 2, if User Answer is partially contained and almost equivalent to Reference Answer\"\n","#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n","#           \"Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics,\"\n","#           \"numbers, metrics, dates and units or the User Answer do not answer the question.\\n\"\n","#           \"Do not explain or justify your rating. Your rating must be only 4, 2 or 0 according to the instructions above.\\n\"\n","#           \"Even small discrepancies in meaning, terminology, directionality, or implication must result in a lower score. Only rate 4 if the User Answer is a complete and precise match to the Reference Answer in every aspect.\\n\"\n","#           \"### Question: {query}\\n\"\n","#           \"### {answer0}: {sentence_inference}\\n\"\n","#           \"### {answer1}: {sentence_true}\\n\"\n","#           \"The rating is:\\n\"\n","#       )\n","#     template_accuracy2 = (\n","#           \"I will rate the User Answer in comparison to the Reference Answer for a given Question.\\n\"\n","#           \"A rating of 4 indicates that the User Answer is entirely consistent with the Reference Answer, covering all aspects, topics, numbers, metrics, dates, and units.\\n\"\n","#           \"A rating of 2 signifies that the User Answer is mostly aligned with the Reference Answer, with minor discrepancies in some areas.\\n\"\n","#           \"A rating of 0 means that the User Answer is either inaccurate, incomplete, or unrelated to the Reference Answer, or it fails to address the Question.\\n\"\n","#           \"I will provide the rating without any explanation or justification, adhering to the following scale: 0 (no match), 2 (partial match), 4 (exact match).\\n\"\n","#           \"Even minor inconsistencies in meaning, terminology, emphasis, or factual detail should prevent a rating of 4. Only assign a 4 if the User Answer exactly and unambiguously matches the Reference Answer in every respect.\"\n","#           \"Do not explain or justify my rating. My rating must be only 4, 2 or 0 only.\\n\\n\"\n","#           \"Question: {query}\\n\\n\"\n","#           \"{answer0}: {sentence_inference}\\n\\n\"\n","#           \"{answer1}: {sentence_true}\\n\\n\"\n","#           \"Rating: \"\n","#       )\n","\n","#     score1 = openai_generator(\n","#                 template_accuracy1.format(\n","#                       query=query,\n","#                       answer0=\"User Answer\",\n","#                       answer1=\"Reference Answer\",\n","#                       sentence_inference=response,\n","#                       sentence_true=reference,\n","#                     )\n","#                 )\n","#     try:\n","#       score1 = int(score1)\n","#     except:\n","#       print(\"Failed\")\n","#       score1 = 0\n","\n","#     score2 = openai_generator(\n","#                 template_accuracy2.format(\n","#                         query=query,\n","#                         answer0=\"Reference Answer\",\n","#                         answer1=\"User Answer\",\n","#                         sentence_inference=reference,\n","#                         sentence_true=response,\n","#                     )\n","#                   )\n","\n","#     try:\n","#       score2 = int(score2)\n","#     except:\n","#       print(\"Failed\")\n","#       score2 = 0\n","\n","\n","#     return (score1 + score2) / 2"],"metadata":{"id":"t89v938Y1o4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 評価\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"CPLGyk7T5LaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZSCNnRf9pJif"},"source":["## 結果 (ベースモデル)\n","\n","Meta-Llama-3-8B-Instructは「Inference Time Scaling」について誤った知識を提示しました：\n","* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"k4R-hiKNGyJd"},"source":["# 2. 文字起こしデータの活用\n","## 講義内容をソースとして活用 (RAG導入)\n","\n","モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n","\n","* **知識ソース**: LLM講座第4講における講師の発言内容\n","* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n","\n","**初期RAG実装（ベーシックアプローチ）**:\n","* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n","* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n","* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n","* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"47GvcceyObAl","executionInfo":{"status":"ok","timestamp":1746595367923,"user_tz":-540,"elapsed":22130,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n","# In case you want to reduce the maximum length:\n","emb_model.max_seq_length = 8192"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"kPwggQfUS5yl","executionInfo":{"status":"ok","timestamp":1746595373872,"user_tz":-540,"elapsed":3,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[],"source":["with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4_raw.txt\", \"r\") as f:\n","  raw_writedown = f.read()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kxzKF6L2THIw","outputId":"62c3b505-b298-4572-cf96-272bcd943e5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595376959,"user_tz":-540,"elapsed":7,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  306\n","ドキュメントの例: \n"," このDecodingにもいろんな方法があってグリーンDecodingだと単純に一番いいやつを選んでいく、一番確率が高いやつ選んでいくので、すごい単純ですけど、こういうトップKeyを取るとかトップ系を取るとかして、最後に一番いいやつを選ぶみたいなことをすると、これも結局計算をたくさんしてることになるわけですね\n"]}],"source":["# ドキュメントを用意する。\n","documents = [text.strip() for text in raw_writedown.split(\"。\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例: \\n\", documents[250])"]},{"cell_type":"code","source":["# Retrievalの実行\n","question = \"LLMにおけるChinchilla scaling lawsとは？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","# 各ドキュメントの類似度スコア\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mN2hdq-GlCvk","executionInfo":{"status":"ok","timestamp":1746595388641,"user_tz":-540,"elapsed":8433,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"2240a0f2-0d65-4ef4-88b1-636df01022f8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[[58.466209411621094, 58.867645263671875, 56.2042236328125, 51.5880126953125, 50.16627883911133, 56.065155029296875, 58.61505126953125, 50.535987854003906, 57.962520599365234, 58.72676467895508, 53.0543212890625, 57.022071838378906, 51.332950592041016, 52.45307540893555, 53.87307357788086, 55.608177185058594, 59.28961181640625, 55.40880584716797, 55.60260009765625, 56.814598083496094, 60.8417854309082, 59.53520584106445, 58.844078063964844, 57.796287536621094, 51.61714553833008, 59.911930084228516, 55.77289962768555, 62.166744232177734, 53.554595947265625, 56.303192138671875, 54.79912185668945, 55.213016510009766, 52.73438262939453, 50.01127624511719, 53.539432525634766, 55.44524383544922, 55.469749450683594, 55.499794006347656, 55.84645080566406, 54.591896057128906, 55.33937072753906, 52.01337432861328, 55.783592224121094, 53.64848327636719, 54.39429473876953, 55.47475814819336, 58.490055084228516, 57.57576370239258, 56.04449462890625, 56.62965774536133, 56.70292282104492, 50.29706954956055, 58.771549224853516, 52.65800857543945, 57.11121368408203, 58.991455078125, 58.411048889160156, 57.171791076660156, 60.67559814453125, 60.299373626708984, 57.571876525878906, 60.78593826293945, 57.401023864746094, 57.550323486328125, 52.86668395996094, 54.247535705566406, 54.17115020751953, 60.50279235839844, 62.59846496582031, 55.28666305541992, 51.32122039794922, 57.073665618896484, 58.75637435913086, 49.840457916259766, 56.46338653564453, 57.23616027832031, 53.25456237792969, 61.958194732666016, 56.04375076293945, 56.6228141784668, 55.4269905090332, 57.55916976928711, 54.72706604003906, 54.44034194946289, 55.62604522705078, 52.82259750366211, 57.45410919189453, 53.00272750854492, 60.187339782714844, 59.02197265625, 49.71165084838867, 55.257354736328125, 58.30811309814453, 62.86857223510742, 56.92790222167969, 53.97679901123047, 56.7753791809082, 53.930294036865234, 55.8701286315918, 55.770809173583984, 51.387943267822266, 55.12212371826172, 56.47319412231445, 55.27035140991211, 58.429378509521484, 57.834537506103516, 48.182315826416016, 50.246002197265625, 55.12815856933594, 56.06985855102539, 48.182315826416016, 49.74527359008789, 54.95500564575195, 59.32377624511719, 54.58403778076172, 50.343345642089844, 57.77299880981445, 50.26960372924805, 53.59261703491211, 59.895877838134766, 56.811851501464844, 49.93498992919922, 53.659236907958984, 49.83198165893555, 56.62187194824219, 51.70439529418945, 55.36429214477539, 48.44076156616211, 52.29037857055664, 56.62208938598633, 56.78542709350586, 59.60740280151367, 52.651466369628906, 56.091583251953125, 53.82846450805664, 52.861793518066406, 55.67198944091797, 57.43986892700195, 52.45350646972656, 56.07373809814453, 55.13900375366211, 51.829063415527344, 64.00057983398438, 60.05733108520508, 56.338966369628906, 56.04658126831055, 55.94134521484375, 50.956390380859375, 57.6749153137207, 57.249366760253906, 56.3727912902832, 62.47687530517578, 54.61720275878906, 56.44169998168945, 62.4739875793457, 55.18220520019531, 50.74455261230469, 54.19304275512695, 50.87199401855469, 53.20899963378906, 50.929927825927734, 54.02919006347656, 53.39487075805664, 50.05119323730469, 57.18682861328125, 49.092166900634766, 60.327877044677734, 56.5297966003418, 58.89763259887695, 55.5091438293457, 53.01226806640625, 58.547645568847656, 57.949493408203125, 53.139854431152344, 59.42829895019531, 56.26639175415039, 52.500118255615234, 59.934200286865234, 58.20936584472656, 63.4155158996582, 49.336822509765625, 55.124847412109375, 60.94901657104492, 49.61035919189453, 57.160491943359375, 51.870216369628906, 59.03379821777344, 51.0915641784668, 51.880516052246094, 56.28072738647461, 52.42367935180664, 55.231346130371094, 52.72352600097656, 56.294464111328125, 50.67050552368164, 58.63029861450195, 56.06275939941406, 53.6107292175293, 51.122718811035156, 55.174659729003906, 54.261051177978516, 53.54907989501953, 56.90079879760742, 52.52228546142578, 55.767120361328125, 55.61698532104492, 49.71333312988281, 56.59341812133789, 56.2138671875, 57.64863586425781, 57.734256744384766, 50.18756866455078, 58.64249038696289, 56.21495819091797, 56.122676849365234, 56.17396926879883, 60.528480529785156, 61.4248161315918, 54.79402542114258, 54.182891845703125, 52.297950744628906, 58.13919448852539, 57.5881233215332, 61.873756408691406, 57.543609619140625, 55.13422393798828, 56.267578125, 54.71390914916992, 54.875648498535156, 54.251373291015625, 55.03972625732422, 54.974185943603516, 55.51762390136719, 52.0073356628418, 61.836116790771484, 57.914066314697266, 50.86377716064453, 56.97602081298828, 53.8258056640625, 52.63431167602539, 57.27263259887695, 55.280452728271484, 60.468292236328125, 54.19670867919922, 60.77754592895508, 55.55925369262695, 52.67128372192383, 50.47813034057617, 58.588443756103516, 52.3538818359375, 53.463661193847656, 54.010772705078125, 50.504295349121094, 49.064208984375, 55.67724609375, 51.75703811645508, 59.85898208618164, 52.603755950927734, 53.78096008300781, 48.604454040527344, 51.44231414794922, 51.14853286743164, 50.59504699707031, 48.26477813720703, 58.85182571411133, 50.819725036621094, 55.252967834472656, 54.47237014770508, 53.8597412109375, 55.025028228759766, 51.57539367675781, 52.68284225463867, 51.05958557128906, 54.929500579833984, 55.798316955566406, 49.79839324951172, 53.70695114135742, 52.59097671508789, 52.087711334228516, 54.40040969848633, 51.50023651123047, 55.779083251953125, 53.2953987121582, 56.232200622558594, 53.70751190185547, 47.87445068359375, 54.021366119384766, 52.9650764465332, 51.06517028808594, 52.729068756103516, 55.040252685546875, 51.1812858581543, 51.64527893066406, 50.68202590942383, 52.07740020751953, 52.95062255859375, 51.434200286865234, 53.35449981689453, 52.582157135009766, 55.04853820800781, 58.193023681640625, 53.254844665527344, 57.63574981689453, 64.78422546386719, 51.716732025146484, 48.28649139404297]]\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"b_v8gx_tTHIx","outputId":"bb4233cb-f2b3-418b-8bfd-60482830197d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595392968,"user_tz":-540,"elapsed":12,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 64.78422546386719)\n","1月に論文としてまして、スケーリングLLMthisTimeコンピュート口真理ちゃん日は増えてるっていうことで、良いらしいというふうに言われてます \n","\n","\n","取得したドキュメント2: (Score: 64.00057983398438)\n","この辺の理屈は小さなモデルだと、学習がロスが下がらなくなるというのでちょっとあのスケール則を書いてるときに、なんか言っこのプロット説明者と同じような話ですけど、あの計算量が与えられたときに、どうやら、別にパラメータを増やせばいいわけではないいうことはわかりこの計算量が与えられたときにどれぐらいのバジェット、どれぐらいのパラメータ数とtoken数に割り振ればいいのかっていうのを、あの計算しようとしたのが、先ほど出したこの2変数の関係っていうふうに言ったChinChillaと呼ばれる経験則になってます \n","\n","\n","取得したドキュメント3: (Score: 63.4155158996582)\n","これがあの係数が20このtokenをパラメータであった値が、20、20倍のtokenを利用しましょうということでしたけど、例えばLlama2だと学び7Billonでも70Billonと同じtokenを使ってますけど、7Billonのものに関しては1.8とBillonのtoken使ってるので、285倍のケース全然違う値を使ってるDense70Billonは28個Llama3に関しても、21470Billonが214倍、400倍Billonの方が37倍ということで、このChinChillaオプティカル実際に巨大なモデル、巨大なtokenの学習を使っているケースもよくあります \n","\n","\n","取得したドキュメント4: (Score: 62.86857223510742)\n","それから1変数を制御するのではなくて複数の変数を制御するような経験則も知られていて有名なBERTChinChillaと呼ばれる経験則がありますChinChilla論文って確かにここまでで計測スケール則の話をしたんですけどChinChillaのところで少しパラメータ数とデータセットサイズ、それぞれいじって計算量を肯定しますよって話をしたのでちょっとその補足をしておきたいと思います \n","\n","\n","取得したドキュメント5: (Score: 62.59846496582031)\n","Trasnformerの場合はスケール則がごめんなパラメータ数が横軸になってますけどこういうふうになると、LLMの場合の一掃にソヨンそうなのでそれぞれ計測と書くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの研修をされて、深さについても検証してまして、これも他のモデルが何だったかちょっと忘れちゃったけどリスキーだったような気がしますけどそう変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなGENIACすることができます \n","\n","\n"]}],"source":["topk = 5\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"]},{"cell_type":"code","source":["references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0pIN_E1mG73","executionInfo":{"status":"ok","timestamp":1746595408087,"user_tz":-540,"elapsed":12325,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"b09ea88c-7969-4c7b-d519-4b7a321a409e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"xzeZErLk6cAH","outputId":"5233ad9a-f545-4b27-c2cf-6c9eefcb9da9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595418517,"user_tz":-540,"elapsed":11,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、Chinchillaという経験則に基づいて、パラメータ数、token数、計算量の関係を規定するスケール則です。\n","\n","Chinchilla scaling lawsは、LLMの学習に必要な計算量を、パラメータ数とtoken数の関係で規定することで、計算量を予測することができます。具体的には、Chinchilla scaling lawsは、パラメータ数とtoken数の関係を、以下の式で規定します。\n","\n","* パラメータ数：P\n","* token数：T\n","* 計算量：C\n","\n","この式で、計算量Cは、パラメータ数Pとtoken数Tの積に比例することが示されます。すなわち、パラメータ数Pを増やすことで、token数Tを増やすことで、計算量Cを減らすことができます。\n","\n","Chinchilla scaling lawsは、LLMの学習に必要な計算量を規定することで、計算量を予測することができます。また、Chinchilla scaling lawsは、LLMのパラ\n"]}]},{"cell_type":"code","source":["# # 評価 (openai apikeyがある場合のみ実行)\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"eEc_mCqp6npE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn7tih0RTTzr"},"source":["## 結果 (初期RAG実装)\n","\n","講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n","* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n","* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n","\n","### 問題分析\n","以下の要因が考えられます：\n","1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n","2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性\n","\n","### 書き起こしテキストの品質改善\n","\n","日本語の音声認識（speech2text）モデルは精度に課題があることが知られています。以下に「LLMにおけるInference Time Scalingとは？」に関連する講義内容の書き起こしテキストを比較します："]},{"cell_type":"markdown","metadata":{"id":"q83QyfAIphk6"},"source":["### 講義中の該当発言 (LLM講座Day4後半から抜粋)\n","\n","\n","<修正前>\n","---\n","\n","講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で**生のGENIACLM**の話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、**ゲームソフトの問題は何だと思いますか**って聞かれたとき、多分あの考えることが違うと思うんですね。**羽の色なんですか**っていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**スケール足の問題なんだと思いますか**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**スケールって言ってる7Gのスケール**って言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**大湾**と呼ばれる論文が論文じゃないか、モデルが**オペル**から出ましたプレビューとして出てますけどこの法案で注目されていますこれあの**論文にROMってかブログ**にあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、**水温時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。\n","\n","\n","<修正後>\n","---\n","\n","\n","講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで**「推論時のスケーリング」**についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、**「スケール則の問題は何だと思いますか」**って聞かれたとき、多分あの考えることが違うと思うんですね。\n","**「バナナの色なんですか」**っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**「スケール則の問題なんだと思いますか」**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。\n","なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**推論時のスケールって言ってるのは**こういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。\n","これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**o1**と呼ばれるモデルが**OpenAI**から出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の**論文ってかブログ**にある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、**推論時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n","こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qCrp81WzyhYc"},"source":["---\n","### 文字起こしの誤り\n","\n","上記の比較からわかるように、音声認識による書き起こしには重大な誤りが多数含まれています：\n","* 「スケール則の問題」→「ゲームソフトの問題」\n","* 「o1」→「大湾」\n","といった明らかに文脈に合わない単語変換が発生しています。\n","\n","`LLM2024_day4_raw.txt`の中には、このような誤変換が多数見られます。これらの誤りはRAG性能に直接影響し、モデルの回答精度を低下させる要因となります。\n","\n","したがって、**ドキュメント品質の改善**を行い、RAG性能の向上を図ります。\n","\n","## 講義内容をソースとして活用：改善版RAG実装\n","\n","* **ドキュメント処理**:\n","  - speech2textによる書き起こしテキストを人手で丁寧に修正\n","  - 専門用語（Inference Time Scaling、GPT-o1など）の正確な表記を確保\n","  - 文脈の流れを維持しつつ、文法的に正確な日本語に修正\n","\n","* **検索手法**:\n","  - 引き続き「。」（句点）で区切られた文単位でテキストを分割\n","  - 文単位の検索により、モデルの入力トークン制限内で関連情報を最大化\n","\n","この改善により、モデルが正確な情報に基づいて「Inference Time Scaling」の概念を理解し、適切な回答を生成することが期待されます。"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"WNjIC4RnzkNW","executionInfo":{"status":"ok","timestamp":1746595429785,"user_tz":-540,"elapsed":43,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[],"source":["with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\", \"r\") as f:\n","  raw_writedown = f.read()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"f53OojeTzkNW","outputId":"0b9a95e1-4463-4906-f772-2b6629a94b9b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595432209,"user_tz":-540,"elapsed":9,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  350\n","ドキュメントの例: \n"," それからBest of Nとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2でやったSelf-Consistencyをこの枠組みの一つとして説明されます\n"]}],"source":["# ドキュメントを用意する。\n","documents = [text.strip() for text in raw_writedown.split(\"。\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例: \\n\", documents[310])"]},{"cell_type":"code","source":["# Retrievalの実行\n","question = \"LLMにおけるChinchilla scaling lawsとは？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","# 各ドキュメントの類似度スコア\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jv4eISGRnF2u","executionInfo":{"status":"ok","timestamp":1746595497920,"user_tz":-540,"elapsed":7160,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"ea0cd4a9-029f-4a65-eaa3-8f5d75fbe6b9"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[[58.231544494628906, 58.551658630371094, 56.95147705078125, 49.53276824951172, 52.455074310302734, 50.16627502441406, 56.065185546875, 58.466102600097656, 51.71729278564453, 53.074241638183594, 57.37200927734375, 58.72675323486328, 53.23201370239258, 57.02206039428711, 51.33295822143555, 52.45307540893555, 53.87309646606445, 49.60065460205078, 56.42440414428711, 59.28964614868164, 55.4088249206543, 57.249534606933594, 56.814605712890625, 60.841796875, 61.37364959716797, 57.1976432800293, 55.24169158935547, 59.148372650146484, 51.61715316772461, 57.661991119384766, 53.171356201171875, 57.68410110473633, 62.166778564453125, 53.21492004394531, 56.30318069458008, 54.628814697265625, 56.567893981933594, 53.73492431640625, 53.53944396972656, 55.44525909423828, 55.92243576049805, 52.791778564453125, 53.48512649536133, 55.95191192626953, 55.62403106689453, 53.648502349853516, 54.69175720214844, 52.57651901245117, 54.51872253417969, 50.154727935791016, 58.50096893310547, 60.316932678222656, 56.0499382019043, 58.92292404174805, 57.73064041137695, 54.66755676269531, 53.13259506225586, 56.06156539916992, 56.67595672607422, 56.408203125, 58.51138687133789, 55.093955993652344, 60.89276123046875, 54.81792068481445, 58.43427658081055, 55.863441467285156, 52.45419692993164, 57.79872131347656, 59.157432556152344, 55.23780059814453, 53.40412902832031, 58.1004753112793, 62.53028106689453, 53.53321838378906, 57.70487976074219, 61.97057342529297, 57.32412338256836, 57.87232971191406, 51.77904510498047, 54.24753952026367, 56.113834381103516, 60.17619323730469, 61.44133377075195, 55.63816452026367, 55.28667449951172, 51.32122802734375, 56.76227951049805, 59.84195327758789, 57.688785552978516, 51.95561218261719, 58.56950378417969, 56.96283721923828, 54.43507385253906, 58.912349700927734, 56.46170425415039, 57.24944305419922, 56.555747985839844, 56.50928497314453, 57.559120178222656, 54.72705841064453, 54.44035339355469, 55.9117317199707, 52.822608947753906, 57.45410919189453, 53.00273895263672, 61.16707992553711, 59.503318786621094, 56.29396438598633, 55.64362335205078, 58.30810546875, 61.070919036865234, 60.27253723144531, 64.91248321533203, 56.901920318603516, 55.23557662963867, 56.77532958984375, 54.13582229614258, 56.014366149902344, 55.62205505371094, 51.38794708251953, 52.49058151245117, 55.41854476928711, 57.42404556274414, 54.262001037597656, 54.23824691772461, 54.47352600097656, 57.83454132080078, 48.18231964111328, 50.246002197265625, 54.92079162597656, 56.06986999511719, 48.18231964111328, 48.755985260009766, 55.92251968383789, 56.398155212402344, 56.30129623413086, 55.128448486328125, 51.72748565673828, 56.889217376708984, 50.26960372924805, 53.313053131103516, 57.598411560058594, 56.811859130859375, 49.93497848510742, 53.00175094604492, 49.83200454711914, 56.09441375732422, 51.85675811767578, 56.62602233886719, 56.78544616699219, 59.65250015258789, 54.175453186035156, 54.525821685791016, 55.666778564453125, 53.968666076660156, 56.17365264892578, 58.713104248046875, 48.860164642333984, 51.52356719970703, 54.13275909423828, 51.93537521362305, 54.46421432495117, 54.401737213134766, 56.066131591796875, 51.90071487426758, 63.949031829833984, 60.48332595825195, 56.14387893676758, 56.216041564941406, 58.64822769165039, 50.956390380859375, 57.638858795166016, 54.444026947021484, 50.06398010253906, 58.93756103515625, 55.74678039550781, 62.39685821533203, 54.617191314697266, 58.19194412231445, 62.47397232055664, 55.235313415527344, 50.74458694458008, 54.19304275512695, 54.05862808227539, 55.45059585571289, 54.07016372680664, 53.446746826171875, 61.695396423339844, 56.405216217041016, 59.61092758178711, 56.52978515625, 58.63544845581055, 55.85089874267578, 54.103607177734375, 57.447017669677734, 55.6912841796875, 54.7041015625, 61.10858154296875, 56.26640319824219, 52.56220245361328, 59.93418884277344, 58.209373474121094, 53.783714294433594, 61.19837951660156, 55.169952392578125, 63.88928985595703, 58.417945861816406, 53.138065338134766, 54.734046936035156, 61.995361328125, 58.92464065551758, 51.49455261230469, 60.0377197265625, 49.58556365966797, 54.43598175048828, 56.280731201171875, 51.2685432434082, 54.530860900878906, 55.33342361450195, 53.51921844482422, 54.95884323120117, 51.00415802001953, 57.467079162597656, 55.9730224609375, 54.77063751220703, 51.93156051635742, 50.86674499511719, 55.174659729003906, 56.03676986694336, 53.549076080322266, 56.90081024169922, 53.13661193847656, 54.50188064575195, 59.99254608154297, 51.54285430908203, 56.2138671875, 57.07422637939453, 58.50298309326172, 50.18756866455078, 59.91412353515625, 56.2149658203125, 56.01023483276367, 56.27924728393555, 54.65303039550781, 62.16216278076172, 59.050697326660156, 62.291439056396484, 55.754398345947266, 56.59622573852539, 52.297950744628906, 59.60125732421875, 57.20431900024414, 61.428672790527344, 57.808921813964844, 55.49371337890625, 57.543609619140625, 55.134220123291016, 56.80862045288086, 54.71390914916992, 57.84598159790039, 51.64143371582031, 55.25497055053711, 57.14303970336914, 55.1527214050293, 54.117210388183594, 55.35882568359375, 53.54245376586914, 62.061744689941406, 57.129085540771484, 57.52873229980469, 50.86380386352539, 58.809207916259766, 54.14261245727539, 49.425716400146484, 56.666587829589844, 52.634300231933594, 57.3504753112793, 55.27019500732422, 54.517250061035156, 50.32124328613281, 60.28692626953125, 59.47136688232422, 54.621360778808594, 60.25366973876953, 53.95705795288086, 53.46668243408203, 51.73093032836914, 58.6342887878418, 52.3538932800293, 54.617713928222656, 54.01074981689453, 50.504295349121094, 49.064212799072266, 55.004119873046875, 51.754188537597656, 55.44880676269531, 53.82999801635742, 50.223331451416016, 52.359371185302734, 51.193660736083984, 50.52987289428711, 54.74510955810547, 49.46232986450195, 54.56980514526367, 49.7081298828125, 58.415557861328125, 51.38704299926758, 54.94438552856445, 53.17301940917969, 53.85975646972656, 53.63302230834961, 52.60044479370117, 54.172752380371094, 52.88911437988281, 54.05061340332031, 51.505435943603516, 53.35108184814453, 53.800437927246094, 53.37471389770508, 53.38727951049805, 53.21019744873047, 55.54890060424805, 54.10260009765625, 56.042640686035156, 53.63798141479492, 52.91274642944336, 52.51380920410156, 51.06517028808594, 53.04505920410156, 52.93144989013672, 53.75400924682617, 51.18129348754883, 51.80754852294922, 55.0135612487793, 51.01752471923828, 52.80989456176758, 50.48848342895508, 52.58848571777344, 51.922630310058594, 51.551841735839844, 53.67974090576172, 52.100284576416016, 51.436519622802734, 54.63994598388672, 52.052940368652344, 58.913970947265625, 52.1130256652832, 61.15900421142578, 51.973331451416016, 48.28647232055664]]\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"id":"FNsGUsnlOoMm","outputId":"dc667667-d950-4a02-9a55-8367eb18a368","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595506115,"user_tz":-540,"elapsed":5,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 64.91248321533203)\n","ここまでで計測スケール則の話をしたんですけどChinChillaのところで少しパラメータ数とデータセットサイズ、それぞれいじって計算量を固定しますよって話をしたのでちょっとその補足をしておきたいと思います \n","\n","\n","取得したドキュメント2: (Score: 63.949031829833984)\n","この辺の理屈は小さなモデルだと、学習がロスが下がらなくなるというのでちょっとあのスケール則を書いてるときに、なんか1個1個のプロットを説明したと同じような話ですけど、あの計算量が与えられたときに、どうやら、別にパラメータを増やせばいいわけではないいうことはわかりこの計算量が与えられたときにどれぐらいのバジェット、どれぐらいのパラメータ数とtoken数に割り振ればいいのかっていうのを、あの計算しようとしたのが、先ほど出したこの2変数の関係っていうふうに言ったChinChillaと呼ばれる経験則になってます \n","\n","\n","取得したドキュメント3: (Score: 63.88928985595703)\n","Llama3に関しても、70Billonが214倍、400Billonの方が37倍ということで、このChinChilla-Optimalが実際に巨大なモデル、巨大なtokenの学習で使っているケースもよくあります \n","\n","\n","取得したドキュメント4: (Score: 62.53028106689453)\n","あのスケール則についてここまで他の多分一番有名なのが先ほどのから話しているScaling Laws for Neural Language Modelというものなんですけれど、実はこのスケーリングっていうの自体、スケーリング則、スケール則が成立するってこと自体は、もうちょっと前から知られていたというふうに言われています \n","\n","\n","取得したドキュメント5: (Score: 62.47397232055664)\n","ちなみにこれも余談ですけどこのの求め方このChinChilla則ってのは実は何かいくつかの方法で求められてまして、それぞれ大体同じような経験則が出るってことが知られています \n","\n","\n"]}],"source":["topk = 5\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"]},{"cell_type":"code","source":["references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbdQcgQ4ncC_","executionInfo":{"status":"ok","timestamp":1746595518583,"user_tz":-540,"elapsed":9185,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"bfac8ce8-a7ac-4886-96f0-d1fbf196c95f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","execution_count":23,"metadata":{"id":"2FbzMLfTtWxx","outputId":"77def016-cca6-48c8-b573-a6052f1467c2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746595524395,"user_tz":-540,"elapsed":5,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、ChinChillaという経験則に基づくスケーリング則です。このスケーリング則は、計算量を固定した状態で、パラメータ数とデータセットサイズを変化させることで、LLMの性能を予測するために使用されます。\n","\n","このスケーリング則は、ChinChillaの経験則に基づいており、計算量を固定した状態で、パラメータ数とデータセットサイズを変化させることで、LLMの性能を予測するために使用されます。このスケーリング則は、LLMの巨大化に伴う計算量の増加を考慮し、パラメータ数とデータセットサイズを適切に設定することで、LLMの性能を向上させることができます。\n"]}],"source":["print(response)"]},{"cell_type":"code","source":["# # 評価 (openai apikeyがある場合のみ実行)\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"9nBfE52t7jJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLe0IJPeH97d"},"source":["## 結果 (修正テキストによるRAG)\n","\n","書き起こしテキストの品質改善により、モデルの回答に部分的な向上が見られました：\n","\n","### 改善点\n","* 「推論時（Inference）に計算資源をスケーリングすることで、モデルがより賢くなり、性能が向上すること」という概念を正確に捉えるようになった\n","\n","### 問題点\n","* 「Inference Time Scalingは、TransformerやLSTMなどのモデルにおいて、パラメータ数を増やすのではなく、推論時計算資源をスケーリングすることで、性能が向上すること...」という記述は講義内容と矛盾している\n","\n","### 問題分析\n","\n","モデルが誤った回答を生成する主要因として、**文脈の欠如**が考えられます：\n","* 「。」で区切られた短い文単位での検索では、各文の発言背景や関連性が失われる\n","* 単独の文から情報を抽出するため、講師の全体的な主張や議論の流れを把握できない\n","* 結果として、正しい個別の文でも、その解釈に必要な背景情報が欠如し、誤った文脈で理解される"]},{"cell_type":"markdown","source":["# 3. 文脈を考慮したチャンク化の導入\n","\n","検索結果の品質向上のため、以下の改善を実施します：\n","\n","* **前後文脈を含むチャンク化**:\n","  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n","  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n","  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n","\n","* **期待される効果**:\n","  - 講師の主張とその根拠の関係性を正確に把握できる\n","  - 概念の定義とその適用範囲を正しく理解できる\n","\n","この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"],"metadata":{"id":"cydr_gASBU7h"}},{"cell_type":"code","source":["# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n","references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVzRxbVeoYox","executionInfo":{"status":"ok","timestamp":1746595608026,"user_tz":-540,"elapsed":10778,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"b6a18e4a-c8e9-4588-fb85-1739fcd1ee86"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JA6wcZEUodZM","executionInfo":{"status":"ok","timestamp":1746595614792,"user_tz":-540,"elapsed":9,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"a676a32a-16c7-4638-8511-615c39bb5b4c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、ChinChilla則という経験則に基づくスケーリング則です。この則は、巨大なモデルにおいて、パラメータ数、token数、計算量の関係を捉えるために用いられます。\n","\n","ChinChilla則は、Chinchillaというモデル名に由来し、ChinChilla-Optimalという名称で知られています。この則は、巨大なモデルにおいて、パラメータ数を20倍にすると、token数を28.5倍にすると、計算量が変わらないことを示しています。\n","\n","この則は、LLMの巨大化に際して、パラメータ数、token数、計算量の関係を捉えるために重要な指標となります。ChinChilla則は、実際のLLMの巨大化において、パラメータ数、token数、計算量の関係を捉えるために用いられます。\n"]}]},{"cell_type":"code","source":["# # 評価 (openai apikeyがある場合のみ実行)\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"aONZ9fT4Bd0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CD3R54G1WX8B"},"source":["## 結果 (文脈付きチャンク化によるRAG)\n","\n","文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n","\n","### 改善点\n","* 「推論時の計算をスケールさせる」という概念を据えて回答\n","* Inference Time Scalingの基本原理についての理解が向上\n","\n","### 残存する問題点\n","* 質問と関連性の低い情報（ノイズ）が混入する\n","\n","### 問題分析\n","\n","文脈付きチャンク化によるアプローチで新たに発生した課題：\n","\n","1. **情報過多の問題**:\n","   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n","   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n","\n","2. **情報選択の複雑化**:\n","   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n","   * この二重タスクにより回答生成の難易度が上昇"]},{"cell_type":"markdown","source":["# 4. Rerankによる情報品質の向上\n","\n","検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n","\n","* **Rerank手法の導入**:\n","  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n","  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n","  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n","  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n","\n","* **期待される効果**:\n","  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n","  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n","  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n","\n","この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"],"metadata":{"id":"sedbYERHBmx1"}},{"cell_type":"markdown","metadata":{"id":"Fs74h4ADXj99"},"source":["上記より、上位3件のみが関連しているとわかったので、これらだけをモデルに渡すこととする。"]},{"cell_type":"code","source":["# # 評価 (openai apikeyがある場合のみ実行)\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"6Wo1v-V7CEBv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n","references = []\n","for ref in [\"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n","\n","  system_prompt = \"与えられた参考資料が質問に直接関連しているか？'yes''no'で答えること。ただし、余計なテキストを生成しないこと。\"\n","  question =  f\"[参考資料]\\n{ref}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","  response = generate_output(question, system_prompt)\n","\n","  print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n","  print(\"\\n関連しているかどうか: \", response)\n","\n","  if \"yes\" in response.lower():\n","    references.append(ref)\n","\n","  print(\"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGkgw3rFp8BP","executionInfo":{"status":"ok","timestamp":1746595685969,"user_tz":-540,"elapsed":1206,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"3988914f-df56-4730-d3bd-fa534301b8ac"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," それから1変数を制御するのではなくて複数の変数を制御するような経験則も知られていて、有名なのではChinChillaと呼ばれる経験則があります。\n","ChinChilla論文って呼ぶ方が正しいかもしれない。\n","ここまでで計測スケール則の話をしたんですけどChinChillaのところで少しパラメータ数とデータセットサイズ、それぞれいじって計算量を固定しますよって話をしたのでちょっとその補足をしておきたいと思います。\n","これよく出てくる式、あの経験則の近似式なんですけど、学習に必要な計算量ってどうやって計算してるんですかっていう話があると思います\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," それから逆に右側が横軸がコンピュートになっていて、色がモデルサイズであることは変わらないんですけどこれを見ると例えば10のマイナス3乗の計算量があるときには、これぐらいのコンピュータを使えばいいということがわかったりします。\n","これ別に大きければいいというわけではないと。\n","この辺の理屈は小さなモデルだと、学習がロスが下がらなくなるというのでちょっとあのスケール則を書いてるときに、なんか1個1個のプロットを説明したと同じような話ですけど、あの計算量が与えられたときに、どうやら、別にパラメータを増やせばいいわけではないいうことはわかりこの計算量が与えられたときにどれぐらいのバジェット、どれぐらいのパラメータ数とtoken数に割り振ればいいのかっていうのを、あの計算しようとしたのが、先ほど出したこの2変数の関係っていうふうに言ったChinChillaと呼ばれる経験則になってます。\n","ChinChillaはモデルの名前なんですけどなんかChinChilla則って言われたり、ChinChillaケースって言われたりするので、何かその辺を丸ごと足してChinChillaというふうに、大体呼ばれてると思えばと思います\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," 20倍のtokenを利用しましょうということでしたけど、例えばLlama2だと学び7Billonでも70Billonと同じtokenを使ってますけど、7Billonのものに関しては1.8Trillionのtoken使ってるので、285倍の係数と全然違う値を使ってる。\n","70Billonは28.5倍。\n","Llama3に関しても、70Billonが214倍、400Billonの方が37倍ということで、このChinChilla-Optimalが実際に巨大なモデル、巨大なtokenの学習で使っているケースもよくあります。\n","ここまでのまとめが、スケール則ををどう活用するかって話をしてきました\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," 黄色が一番大きくて青が一番ちっちゃい、紫かな、ちっちゃいモデルになってます。\n","先行研究、さっきの今まで話してたものより2桁オーダーが大きいモデルにおいても、これ厳密に言うとまだ収束してないように見えるんですけど、おおむねスケール則が成立してるっていうことがGPT3の論文だと報告されています。\n","あのスケール則についてここまで他の多分一番有名なのが先ほどのから話しているScaling Laws for Neural Language Modelというものなんですけれど、実はこのスケーリングっていうの自体、スケーリング則、スケール則が成立するってこと自体は、もうちょっと前から知られていたというふうに言われています。\n","ちょっと僕ももしかしたらもっと昔からあるかもしれないんで僕が知ってる限りですが、少なくとも2017年の論文では検証されているということが言われてます\n","\n","関連しているかどうか:  no\n","\n","\n","\n","\n","\n","対象となるドキュメント:\n"," 結果としてはこれで多くのケースより巨大なモデルに勝てるということが実験上示されています。\n","これ左側のやつが実験上の結果じゃないんで、ちょっと実験結果飲みたい人はこの元の論文を見てもらえれば良いと思いますけど、あの巨大なモデルにかかってるということでこの関係性が良さそうだということが示されています。\n","ちなみにこれも余談ですけどこのの求め方このChinChilla則ってのは実は何かいくつかの方法で求められてまして、それぞれ大体同じような経験則が出るってことが知られています。\n","この関係式っていうのがよくこれも知られてまして、大体最適なtoken数っていうのが、パラメータ数に20をかけたもの\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]}]},{"cell_type":"code","source":["print(len(references))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziLbJL6LqKOY","executionInfo":{"status":"ok","timestamp":1746595694653,"user_tz":-540,"elapsed":6,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"2bba5e70-19c4-4833-8ee7-0b1134727649"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"code","source":["system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfm43xpwqNa9","executionInfo":{"status":"ok","timestamp":1746595710138,"user_tz":-540,"elapsed":12105,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"33b6f813-29c1-43b2-d946-c4aa53660e27"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfmBCNHfqaVh","executionInfo":{"status":"ok","timestamp":1746595734522,"user_tz":-540,"elapsed":44,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"6d719dfa-b416-4c1f-d13f-3efa58c9ec22"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、ChinChillaと呼ばれる経験則に基づくスケール則です。この経験則は、巨大なモデルや巨大なトークンに対する学習の計算量を予測するために使用されます。\n","\n","この経験則は、ChinChillaの論文に基づいており、パラメータ数とトークン数の関係を示しています。具体的には、パラメータ数を20倍にすると、トークン数も20倍になるという関係を示しています。\n","\n","この関係式は、LLMの学習に必要な計算量を予測するために使用されます。例えば、巨大なモデルに対する学習には、より多くのトークンが必要になるため、この関係式を使用して、トークン数を計算することができます。\n","\n","また、この経験則は、LLMの性能を向上させるために使用されます。例えば、パラメータ数を20倍にすると、トークン数も20倍になるため、LLMの性能も向上することができます。\n"]}]},{"cell_type":"markdown","metadata":{"id":"elqD2gJt5RCo"},"source":["## 結果 (Rerank導入後)\n","\n","Rerankの導入により、回答品質に改善が見られました：\n","\n","### 達成された成果\n","* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n","* 無関係な情報やノイズの排除\n","* 講義内容を反映した説明の実現 🎉\n","\n","この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。\n","\n","---\n","\n","\n","# 5. さらなる改善案: 意味的チャンク化\n","\n","文単位での分割と前後文脈の追加という現在のアプローチをさらに発展させる手法として、**意味的なチャンク化**が考えられます：\n","\n","* **意味的チャンク（段落）単位での分割**:\n","  - 単純な文の区切りではなく、意味的なまとまり（トピック、議論、例示など）に基づいてテキストを分割\n","  - 人間の主観に基づく意味的な段落分けを活用\n","  - 各チャンクが「一つの完結した考え」を表現するようにする\n","\n","* **期待される効果**:\n","  - より自然な文脈理解が可能に（人間の思考や会話の流れに近い）\n","  - トピックの開始から結論までの流れを維持できる\n","  - 概念間の関係性や比較が同一チャンク内に含まれ、より深い理解につなげる\n","\n","* **検証方法**:\n","  - 人間が主観的に意味でグループ化したチャンクセットを用意\n","  - 同じRerank手法を適用し、文単位チャンクとの性能差を比較\n","  - 回答の正確性、一貫性、網羅性を評価指標として使用\n","\n","この意味的チャンク化手法は、特に講義のような構造化された発話においては、より自然で効果的な情報検索と理解を可能にすると予想されます。"]},{"cell_type":"code","source":["# 本来は段落をそのままdocumentsに入れずに一定のサイズに分割した方が良いでしょうが、簡単のために段落をそのまま入れてしまいます。\n","documents = [text.replace(\"\\n\", \" \").strip() for text in raw_writedown.split(\"\\n\\n\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例: \\n\", documents[30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NesMP7HyrBn9","executionInfo":{"status":"ok","timestamp":1746595779415,"user_tz":-540,"elapsed":16,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"3b914810-28a6-477a-acaa-6d93c6d99126"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  45\n","ドキュメントの例: \n"," 具体的な求め方についても話します。 さっきからチラチラ言ってた通りなんすけど基本的にこれどう図るかっていうと、基本的にはいくつかの条件で実験してフィッティングするって言ってんのは、すごい単純に言ってしまうとそうなります。左側GPT4の論文から取ってきた図で説明したもんですけど、グレーのやつを例えば実験してみて、これぐらいのロスになるんだなっていうので、フィッティングするとこういうカーブになります。 ちなみにこれ、なんでこれ直線にならないんだっていうのをすぐ説明しなかったですがこれ縦軸が実は普通のロスと違ってBits-per-wordっていうのになってて、多分2乗スケールのロスになってるからだと思います。 右側も同じですね。この各点について何かいろんな設定で実験してやって、それを結果を見るということをしてますけどよくよく考えるとスケールさせるときにモデルサイズどうすればいいんでしたっけとか、何をどういじるとモデルサイズが大きくなるんでしたっけ、どういうふうに言えばいいんでしたっけとかですね。 あのモデルサイズ変えたときにハイパーパラメータってどうすんでしたっけそういった細かい問題が出てくる。最初の方ですけどモデルサイズどう変化させるかっていうので、前回やった、こういう図があると思いますけどモデルサイズ変えようと思ったら別にパラメータ、層の数を増やしても、いいわけですし、この埋め込みの次元各tokenの次元を増やしてもいいわけですし、各随所に出てくるこのフィードフォワードネットワークっていうのの中間層の次元を上げてもいいですしヘッドを増やしてもそういうのあのパラメータ自体は上がるということで、これどれをどのぐらいやるんですかっていうのが細かく考えると重要になってきます。 この辺は元の論文でも一応議論されてまして、これ三つほど出してるんすけど例えば真ん中のがアスペクト比っていう、モデルのエンベディングのサイズですね。dモデルっていうものを層数で割ったもの、アスペクト比という縦横比みたいなもので幅と深さの比率をアスペクト比っていうふうにこの論文では呼んでいますけど。こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。 気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。 これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います。 それからモデルとフィードフォワードの次元数ですね、モデル次元数に対しフィードフォワードの次元数は3.5倍になっているということがわかります。これ約3.5かな。ちょっと自信ないですちょっとちゃんと計算したとかいった計算したら、ちょっと違ってたら教えてほしいんすけど大体3.5倍ぐらいあったとアテンションのヘッドはこのFFNの次元数と同様にスケールしたモデルの次元と同様にスケールしているということがわかる。 こういった感じで幅とかを大体同じような係数で、なるべく伸ばしてくと、ただこれ、指定したパラメータ数にしようと思ったときに、当然どっかは完全には固定できないので、若干変わりますけど大体同じような比率でスケールさせているというようなことがわかると思います。\n"]}]},{"cell_type":"code","source":["question = \"LLMにおけるChinchilla scaling lawsとは？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBMxeyS5rNHp","executionInfo":{"status":"ok","timestamp":1746595792304,"user_tz":-540,"elapsed":11118,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"9a885c4e-980f-4c59-e065-ba57984dbc05"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[[58.62751007080078, 57.85238265991211, 57.22877883911133, 57.840572357177734, 61.153446197509766, 59.99169158935547, 56.957550048828125, 55.44834899902344, 60.02470779418945, 62.326114654541016, 59.74757766723633, 62.93877410888672, 60.872562408447266, 57.214962005615234, 57.46445083618164, 63.33745574951172, 64.06803131103516, 60.16014862060547, 55.05218505859375, 59.358978271484375, 56.49138259887695, 59.054744720458984, 61.55015182495117, 63.807613372802734, 60.87043380737305, 63.447021484375, 60.91286087036133, 57.454437255859375, 56.79547119140625, 57.59514617919922, 63.183963775634766, 61.365108489990234, 58.448768615722656, 58.96709442138672, 60.12055969238281, 59.38710403442383, 54.35817337036133, 54.19976806640625, 53.446434020996094, 53.69739532470703, 53.49708938598633, 54.25737380981445, 52.76856231689453, 55.05937194824219, 62.04393005371094]]\n"]}]},{"cell_type":"code","source":["# 簡単のためにtop2でやります。結果を見てもらえれば問題なく関連する項目のみ取得できているのが分かるかと思います。\n","topk = 2\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64vdIxXnrP5k","executionInfo":{"status":"ok","timestamp":1746595797473,"user_tz":-540,"elapsed":9,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"eaf287bf-b4aa-4982-e093-ae5a72398367"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 64.06803131103516)\n","ここまでがあのスケール則とは何かのまとめでして、スケール則っていうのは毎回おさらいすると、計算資源とデータセット、パラメータと誤差の間にはこういった経験則がそうですよ。 こういうべき乗則で書けますよっていうのがスケール則でした。両対数グラフで線形なるのは両対数をとってやるとわかるということも説明しました。それから一番有名なのはTrasnformerで本当のスケール則ですけど、それ以外のモデルでも成立しますし、言語以外のタスクでもスケール則ってのは確認されていますという話をしました。 それから1変数を制御するのではなくて複数の変数を制御するような経験則も知られていて、有名なのではChinChillaと呼ばれる経験則があります。ChinChilla論文って呼ぶ方が正しいかもしれない。 \n","\n","\n","取得したドキュメント2: (Score: 63.807613372802734)\n","ChinChillaはモデルの名前なんですけどなんかChinChilla則って言われたり、ChinChillaケースって言われたりするので、何かその辺を丸ごと足してChinChillaというふうに、大体呼ばれてると思えばと思います。 左側の図は先ほど見せたのと同じで、それぞれの色が計算量に相当してまして、パラメータ数を変更させた場合です。右側の図が増えてるんですけど、これを各FLOPSで最適なパラメータに直したものっていうのが、この中央でこれを同じようにデータ(token数)に対して、直したものが中央になります。 例えばこれパラメータ見ると何か例えば3E-21を使えるんだったら、あのこの一番下のやつをピックアップしたやつか、この右側の真ん中の方に行ってきていて、同じように1E-12の場合はぴって引っ張ってくるみたいな、やったときにどういう関係があるかっていうので、これを見てみると何となく大体線形っぽい関係にわかります。 右側がtokenの場合の同様の例ですね。これをフィッティングしてると、例えばこれ適当な値ですけど10^24よりちょっと低いぐらいの計算量が使えますよっていうときには、パラメータ数は63Billonにすればいいと。同じところが、これ取られてるんすけど、データ数がどれぐらいすればいいかっていうと、1.4Trillion tokenにすればいいということがわかります。テラtokenですねごめんなさい。 こうして作られたのがChinChillaと呼ばれるモデルになっています。これあのGopherっていうモデルがありまして、GopherがDeepMindがこの前に出してたモデルで、これが280Billonでtoken数が0.3テラtokenというふうになってます。 要はこいつと比べるとこのChinChillaっていうのはモデルサイズがちっちゃいんだけど、あのトレーニングtokenを増やしたと、それはどうやって決まったかというと先ほど言った通りですけど経験則に基づいてどのぐらいのバジェットをパラメータに割り振ってどれぐらいのバジェットをtokenに割り振るかっていうのを、この経験則によって決めた値を使ってやってやるということをしているものです。 結果としてはこれで多くのケースより巨大なモデルに勝てるということが実験上示されています。これ左側のやつが実験上の結果じゃないんで、ちょっと実験結果飲みたい人はこの元の論文を見てもらえれば良いと思いますけど、あの巨大なモデルにかかってるということでこの関係性が良さそうだということが示されています。 ちなみにこれも余談ですけどこのの求め方このChinChilla則ってのは実は何かいくつかの方法で求められてまして、それぞれ大体同じような経験則が出るってことが知られています。この関係式っていうのがよくこれも知られてまして、大体最適なtoken数っていうのが、パラメータ数に20をかけたもの。 これと同じですね。70Billonを訓練するのに1.4とBillonのtokenを使うということになってますのでこの式の20っていうのも謎のマジックナンバーとしてよく出てくるので覚えておくといいと思います。 先ほどPaLM2の図を見せましたけど。PaLM2でも同じような経験則が成立しますよということが言われています。 \n","\n","\n"]}]},{"cell_type":"code","source":["reference = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるChinchilla scaling lawsとは？\"\n","response = generate_output(question, system_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjDPc3mfrSsG","executionInfo":{"status":"ok","timestamp":1746595839002,"user_tz":-540,"elapsed":12272,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"4480b67f-28fb-44f2-aeeb-65aedd8bac3c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKoUo4EMrU8N","executionInfo":{"status":"ok","timestamp":1746595873072,"user_tz":-540,"elapsed":9,"user":{"displayName":"Bugs B","userId":"11736711679866118393"}},"outputId":"e861deaa-f622-4cd5-9983-faedd79d7dad"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるChinchilla scaling lawsは、ChinChillaと呼ばれる経験則に基づくスケール則です。この経験則は、巨大なモデルや巨大なトークンに対する学習の計算量を予測するために使用されます。\n","\n","この経験則は、ChinChillaの論文に基づいており、パラメータ数とトークン数の関係を示しています。具体的には、パラメータ数を20倍にすると、トークン数も20倍になるという関係を示しています。\n","\n","この関係式は、LLMの学習に必要な計算量を予測するために使用されます。例えば、巨大なモデルに対する学習には、より多くのトークンが必要になるため、この関係式を使用して、トークン数を計算することができます。\n","\n","また、この経験則は、LLMの性能を向上させるために使用されます。例えば、パラメータ数を20倍にすると、トークン数も20倍になるため、LLMの性能も向上することができます。\n"]}]},{"cell_type":"code","source":["# # 評価 (openai apikeyがある場合のみ実行)\n","# score = evaluate_answer_accuracy(question, response, gold_answer)\n","# print(score)"],"metadata":{"id":"so_dSyukCx6d"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b2347474ce7b4392af8185398e0c2f59":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_0836aed21a48439a8080e315889f9102"}},"5ed556d2e9e546cb871fc191f87ef830":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_243190d4f9f7424c8917d94e3daaa1bc","placeholder":"​","style":"IPY_MODEL_96c82411afb64a72ae3a33898bfa2f42","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"c5bd1ed2012048a09d343c67ad36719c":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_6fda146929eb427f8bcf457d0a6f1f20","placeholder":"​","style":"IPY_MODEL_2a08d63f64e948239faff9c585e6b6ce","value":""}},"8fab02b85ba44107a25894c1f7f6ab83":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_329afa3acf454194a21343fc3d46e8c6","style":"IPY_MODEL_c6238b45df974199a2a741c4c0c545cf","value":true}},"1752a457e376443e87ac407cf143e154":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_5f89b4de5da74340963db0adf1b6c30f","style":"IPY_MODEL_9139b66a4e21410cbd0705719a8a22e4","tooltip":""}},"4d9d19b3a5fc4e7dbf4a696509bb0403":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87b873e5060343ec8f70cc16ff3b5492","placeholder":"​","style":"IPY_MODEL_db706ef327ba442992dfab9a4dc30daa","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"0836aed21a48439a8080e315889f9102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"243190d4f9f7424c8917d94e3daaa1bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96c82411afb64a72ae3a33898bfa2f42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fda146929eb427f8bcf457d0a6f1f20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a08d63f64e948239faff9c585e6b6ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"329afa3acf454194a21343fc3d46e8c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6238b45df974199a2a741c4c0c545cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f89b4de5da74340963db0adf1b6c30f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9139b66a4e21410cbd0705719a8a22e4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"87b873e5060343ec8f70cc16ff3b5492":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db706ef327ba442992dfab9a4dc30daa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e022946c478449c7baef8c9c9ec9919a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c2c0570c32b433c8219acad4e63c894","placeholder":"​","style":"IPY_MODEL_96a55fabb715454ab121b7d81b255e71","value":"Connecting..."}},"4c2c0570c32b433c8219acad4e63c894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a55fabb715454ab121b7d81b255e71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b3d5280d3da4f62a690cacddbdbefe3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73278654927e48cc84eb544dce4471dd","IPY_MODEL_5b307ac2315c461fb52e2cead04d9e6f","IPY_MODEL_3ed5258da3cd4741adb99a0e36349fcd"],"layout":"IPY_MODEL_a3a5b151232142d7b1b3dcaaa36f8943"}},"73278654927e48cc84eb544dce4471dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ca7b1039ffb4cd6b735f0e29139778b","placeholder":"​","style":"IPY_MODEL_812674ea6ae64c8a972c2c379bef9d33","value":"tokenizer_config.json: 100%"}},"5b307ac2315c461fb52e2cead04d9e6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85ac1c2bbea8496680c8cde55e23a647","max":50977,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf01474b2dc040b99cd409f285b6531b","value":50977}},"3ed5258da3cd4741adb99a0e36349fcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0983feaa5f0740ee8a19cf92af540cc2","placeholder":"​","style":"IPY_MODEL_40d72d7b76484ee287bfeeb3f0a703f8","value":" 51.0k/51.0k [00:00&lt;00:00, 5.32MB/s]"}},"a3a5b151232142d7b1b3dcaaa36f8943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ca7b1039ffb4cd6b735f0e29139778b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"812674ea6ae64c8a972c2c379bef9d33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85ac1c2bbea8496680c8cde55e23a647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf01474b2dc040b99cd409f285b6531b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0983feaa5f0740ee8a19cf92af540cc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40d72d7b76484ee287bfeeb3f0a703f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b42b399ccbec4d1baef0e346bc941796":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a65b9c35ddc4c72ab31f5dd614535a3","IPY_MODEL_ae947a16873940a9bb041bffdf40d67d","IPY_MODEL_68b98be0df0f4de0870cbd7bca004c08"],"layout":"IPY_MODEL_27384b495e044cb7948602a3812f8dee"}},"2a65b9c35ddc4c72ab31f5dd614535a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_194b20e861a9442ab80f05f0d898dca3","placeholder":"​","style":"IPY_MODEL_29b31a996d2b4c1484c9145cf19e1c77","value":"tokenizer.json: 100%"}},"ae947a16873940a9bb041bffdf40d67d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aee21e50e02490598e86bc499c1f9d8","max":9085698,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7530e8bd4ca3437c92afb201369ad6e8","value":9085698}},"68b98be0df0f4de0870cbd7bca004c08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3d2702862004bff8e2fd708839f874f","placeholder":"​","style":"IPY_MODEL_6881736cf1ff4a9e8a68427b5d5949f1","value":" 9.09M/9.09M [00:00&lt;00:00, 18.7MB/s]"}},"27384b495e044cb7948602a3812f8dee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194b20e861a9442ab80f05f0d898dca3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29b31a996d2b4c1484c9145cf19e1c77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aee21e50e02490598e86bc499c1f9d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7530e8bd4ca3437c92afb201369ad6e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3d2702862004bff8e2fd708839f874f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6881736cf1ff4a9e8a68427b5d5949f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05b9d16280364170aaa267470ac6c04f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e41c161cfb148e09692637a00d3f1e1","IPY_MODEL_e72bcddf80ca4af1a9951b4651f2fe40","IPY_MODEL_3e52da1d701c4fd5bd2caffebe97c5ba"],"layout":"IPY_MODEL_4494c762b50f4223938b95e71327454c"}},"9e41c161cfb148e09692637a00d3f1e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11784e388ab244c69e621b88cb0f7212","placeholder":"​","style":"IPY_MODEL_bbe203b392fd40f8a4a66ee64f4fcd8a","value":"special_tokens_map.json: 100%"}},"e72bcddf80ca4af1a9951b4651f2fe40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47002b4679a9490bbe4f828d6aaa9d34","max":73,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5efeca998801446493598db5300e3ac4","value":73}},"3e52da1d701c4fd5bd2caffebe97c5ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f7c2ac6cc854cd0906d775736b0bc2b","placeholder":"​","style":"IPY_MODEL_0b2b45a3ae7f49628eb51813b0ffe35b","value":" 73.0/73.0 [00:00&lt;00:00, 9.07kB/s]"}},"4494c762b50f4223938b95e71327454c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11784e388ab244c69e621b88cb0f7212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbe203b392fd40f8a4a66ee64f4fcd8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47002b4679a9490bbe4f828d6aaa9d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5efeca998801446493598db5300e3ac4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f7c2ac6cc854cd0906d775736b0bc2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b2b45a3ae7f49628eb51813b0ffe35b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b21f7f5603864f04a2e7d6dc3491d151":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37d8be17b46b40968c2e368768f63324","IPY_MODEL_6f70f26132814dd9a0109d120d7cd96f","IPY_MODEL_f3d25b31cd64478391d979be1f7c4ecf"],"layout":"IPY_MODEL_febec18b49a3400a9aba921135a503d8"}},"37d8be17b46b40968c2e368768f63324":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f498797c8e94014952e0dbfaa8c3307","placeholder":"​","style":"IPY_MODEL_255ae5a0af3e40488f119ef7851548e9","value":"config.json: 100%"}},"6f70f26132814dd9a0109d120d7cd96f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_072180ffb39e4376aa2795feef569057","max":654,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b032aaa06844d84a25c762bacec5c6f","value":654}},"f3d25b31cd64478391d979be1f7c4ecf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f96b04fb9704d99bf6d6a746c12a842","placeholder":"​","style":"IPY_MODEL_ed1f1151372e4494bdb3ff00972dd99b","value":" 654/654 [00:00&lt;00:00, 84.7kB/s]"}},"febec18b49a3400a9aba921135a503d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f498797c8e94014952e0dbfaa8c3307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"255ae5a0af3e40488f119ef7851548e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"072180ffb39e4376aa2795feef569057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b032aaa06844d84a25c762bacec5c6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f96b04fb9704d99bf6d6a746c12a842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed1f1151372e4494bdb3ff00972dd99b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f42e32817a940a0a72ab7243530f8ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fab2ac024124038b49b7e0753466f75","IPY_MODEL_5fb3093e063540c1bb2f897f8e427a06","IPY_MODEL_c89e03b65a54484ead8fce7126fc8023"],"layout":"IPY_MODEL_65f68584dee94214bded0c1b922e5a85"}},"6fab2ac024124038b49b7e0753466f75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90bd5954a445439db9f6f193534d8831","placeholder":"​","style":"IPY_MODEL_46152ffdb9a242a49b8fe9462851093e","value":"model.safetensors.index.json: 100%"}},"5fb3093e063540c1bb2f897f8e427a06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09095a3665bd40a3b0169aa805ae65ff","max":23950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6654dcbde3f046ff8cede5c99e53299a","value":23950}},"c89e03b65a54484ead8fce7126fc8023":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fde6d2192864d6eae0c6a3d2bb93345","placeholder":"​","style":"IPY_MODEL_ae0fbf024d2349238f6ebb9d316e5c65","value":" 23.9k/23.9k [00:00&lt;00:00, 2.59MB/s]"}},"65f68584dee94214bded0c1b922e5a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90bd5954a445439db9f6f193534d8831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46152ffdb9a242a49b8fe9462851093e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09095a3665bd40a3b0169aa805ae65ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6654dcbde3f046ff8cede5c99e53299a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fde6d2192864d6eae0c6a3d2bb93345":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae0fbf024d2349238f6ebb9d316e5c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1869efabe4c1417f8906380aafedec09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a75ad1579863429c8fcb81570eed169d","IPY_MODEL_0022f39d78e548f9ac96d93e916b74d2","IPY_MODEL_ac6763bee19f4f28a56559fbcf5f4715"],"layout":"IPY_MODEL_82c3046b688a4d7f9688ceb62762c440"}},"a75ad1579863429c8fcb81570eed169d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_086898708b1e4c4e95554d7496169320","placeholder":"​","style":"IPY_MODEL_fcbb422ad37746109148a9e339a031a1","value":"Fetching 4 files: 100%"}},"0022f39d78e548f9ac96d93e916b74d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_420ee7f0805841c09ddcb01e5e117087","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_529afedf045b4127817836c6ed51caa7","value":4}},"ac6763bee19f4f28a56559fbcf5f4715":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de0301fb7d644b32ab884eee30672ee6","placeholder":"​","style":"IPY_MODEL_41b04c78fc5644d3ba2f2d817823517a","value":" 4/4 [01:39&lt;00:00, 41.00s/it]"}},"82c3046b688a4d7f9688ceb62762c440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"086898708b1e4c4e95554d7496169320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcbb422ad37746109148a9e339a031a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"420ee7f0805841c09ddcb01e5e117087":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529afedf045b4127817836c6ed51caa7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de0301fb7d644b32ab884eee30672ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b04c78fc5644d3ba2f2d817823517a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8b5ad882e9e4185b948184542839373":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d1e128f69e74ee8891704c77f51a0eb","IPY_MODEL_ea3ad22160dc4ea9befbcc4ad3f752d3","IPY_MODEL_f7e6cbe15a0f4725a89bb808f89d622a"],"layout":"IPY_MODEL_eb5f419438344bb69ef8324f212c0c8a"}},"3d1e128f69e74ee8891704c77f51a0eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_327d4d17454b4e6280afc19143d237a9","placeholder":"​","style":"IPY_MODEL_393f5738216d44558be0490d34290551","value":"model-00002-of-00004.safetensors: 100%"}},"ea3ad22160dc4ea9befbcc4ad3f752d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_242fa28d95004afe8b7a866f983b3091","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1bb5f8db4df4db285c253c610640215","value":4999802720}},"f7e6cbe15a0f4725a89bb808f89d622a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_777914645038483390d10017afda93fe","placeholder":"​","style":"IPY_MODEL_0ccbfe8739b64af6be6590169059f4a7","value":" 5.00G/5.00G [01:39&lt;00:00, 145MB/s]"}},"eb5f419438344bb69ef8324f212c0c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"327d4d17454b4e6280afc19143d237a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"393f5738216d44558be0490d34290551":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"242fa28d95004afe8b7a866f983b3091":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1bb5f8db4df4db285c253c610640215":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"777914645038483390d10017afda93fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccbfe8739b64af6be6590169059f4a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0667bde683824da7baacff3e8cecb609":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23527478133c43a09dad9f82f831d043","IPY_MODEL_7b54e8dd4f6f41d1992b81956e976dbf","IPY_MODEL_caf2e60a294a4facb1d2460fbc7ae97e"],"layout":"IPY_MODEL_e4325e1dc457464e8bee681c0f0edae6"}},"23527478133c43a09dad9f82f831d043":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06b2f559b73b46b9881dbc4455712fae","placeholder":"​","style":"IPY_MODEL_381bb40575ef4c9f9c7407ba24f011a9","value":"model-00001-of-00004.safetensors: 100%"}},"7b54e8dd4f6f41d1992b81956e976dbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7811bde2717444739e179ee9f798c850","max":4976698672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50074d0252974c24a1d786a9ab9186ca","value":4976698672}},"caf2e60a294a4facb1d2460fbc7ae97e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3533b1a0c0044fbabdb646165565f05","placeholder":"​","style":"IPY_MODEL_84e57391650942d687ee9656fbdf9f4f","value":" 4.98G/4.98G [01:38&lt;00:00, 24.7MB/s]"}},"e4325e1dc457464e8bee681c0f0edae6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06b2f559b73b46b9881dbc4455712fae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"381bb40575ef4c9f9c7407ba24f011a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7811bde2717444739e179ee9f798c850":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50074d0252974c24a1d786a9ab9186ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3533b1a0c0044fbabdb646165565f05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84e57391650942d687ee9656fbdf9f4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9f06078d2a5451c908fd7e19507b623":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_252b98f97fde4430b573d734555dc27a","IPY_MODEL_1ff79161fb58432f9434518ffc2e4e2f","IPY_MODEL_48c1a26f723948cf888afcc0271745a3"],"layout":"IPY_MODEL_146ac48e57974d31ab95a2b5526dd486"}},"252b98f97fde4430b573d734555dc27a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cbe3cb201e943018358d5e0c88116cf","placeholder":"​","style":"IPY_MODEL_0c76932a68dd4945b20698c14afea86d","value":"model-00004-of-00004.safetensors: 100%"}},"1ff79161fb58432f9434518ffc2e4e2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb1c7e4b822e4c9a99dbf29686509cea","max":1168138808,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e06da87dff904e4a85801b868b57ddf1","value":1168138808}},"48c1a26f723948cf888afcc0271745a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_103e755ed5c541c8aeef572899fa7e49","placeholder":"​","style":"IPY_MODEL_aac9a79545ac4fb5b0fa82619b0dc099","value":" 1.17G/1.17G [00:12&lt;00:00, 86.7MB/s]"}},"146ac48e57974d31ab95a2b5526dd486":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cbe3cb201e943018358d5e0c88116cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c76932a68dd4945b20698c14afea86d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb1c7e4b822e4c9a99dbf29686509cea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e06da87dff904e4a85801b868b57ddf1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"103e755ed5c541c8aeef572899fa7e49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aac9a79545ac4fb5b0fa82619b0dc099":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"725039c3cd014afbb56f51d2874320d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fa21547710946e7b52527f18a8946ea","IPY_MODEL_d1d087e0143946e99761212b44288e23","IPY_MODEL_b6145ef88515477089a509be5365e415"],"layout":"IPY_MODEL_2e2cf850bca746b3a8605d7456849524"}},"9fa21547710946e7b52527f18a8946ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79e326aa190b46c98e533facfe722b54","placeholder":"​","style":"IPY_MODEL_d3373f2f05324783be993485e5b6e4ca","value":"model-00003-of-00004.safetensors: 100%"}},"d1d087e0143946e99761212b44288e23":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_338b310d399c441795e634957a3a86d9","max":4915916176,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd1f0a7866cf416bb2012120e8192d12","value":4915916176}},"b6145ef88515477089a509be5365e415":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb083e7843764e89bc3befd650dd7164","placeholder":"​","style":"IPY_MODEL_481158b7d84a4bfd9501016e8ac4c43d","value":" 4.92G/4.92G [01:36&lt;00:00, 22.7MB/s]"}},"2e2cf850bca746b3a8605d7456849524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79e326aa190b46c98e533facfe722b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3373f2f05324783be993485e5b6e4ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"338b310d399c441795e634957a3a86d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd1f0a7866cf416bb2012120e8192d12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb083e7843764e89bc3befd650dd7164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"481158b7d84a4bfd9501016e8ac4c43d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52c602196a764485a30154b467639d1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14f05935a2fb4d88bac8dc214ade79a5","IPY_MODEL_288c5229e59c4ff0be28c04cf9249f41","IPY_MODEL_cb0a509423264d26983612af2d14de11"],"layout":"IPY_MODEL_98e86a8e219949ab99243a8966c83fcc"}},"14f05935a2fb4d88bac8dc214ade79a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d31a7ad8c68d4696bcefbe9ca4f983b5","placeholder":"​","style":"IPY_MODEL_8caa3cec07744230b086052f8567eb9a","value":"Loading checkpoint shards: 100%"}},"288c5229e59c4ff0be28c04cf9249f41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_842eaae521174e828b391fd490395bf7","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df98f42be30447e2bfc9913403d60b87","value":4}},"cb0a509423264d26983612af2d14de11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b4ff7f22d7e4db789b471a4008775d9","placeholder":"​","style":"IPY_MODEL_4e5ef4f88cfe4c5f88252288d43e6a2d","value":" 4/4 [00:17&lt;00:00,  3.63s/it]"}},"98e86a8e219949ab99243a8966c83fcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d31a7ad8c68d4696bcefbe9ca4f983b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8caa3cec07744230b086052f8567eb9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"842eaae521174e828b391fd490395bf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df98f42be30447e2bfc9913403d60b87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b4ff7f22d7e4db789b471a4008775d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5ef4f88cfe4c5f88252288d43e6a2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e961e0172894d5aacfd68d169459178":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d270e8befeb43c2956bb48003e35974","IPY_MODEL_2976ac42679c481ca45abfa8e9dd2235","IPY_MODEL_b46654162d084c3e8de4bee41512cb33"],"layout":"IPY_MODEL_8c8cc35b95214f5ab7659e656d9b6134"}},"0d270e8befeb43c2956bb48003e35974":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da809853b1640439e941358a0f38d8e","placeholder":"​","style":"IPY_MODEL_0e283e09688e4febb699f4789be996b7","value":"generation_config.json: 100%"}},"2976ac42679c481ca45abfa8e9dd2235":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_941aa19f00454faf9c0673532fed042f","max":187,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef0eae54863f4af5b978e2b40bc44886","value":187}},"b46654162d084c3e8de4bee41512cb33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7742b64b7f064344bdcabbb73c4bc361","placeholder":"​","style":"IPY_MODEL_cedd1ba46d3949eda270cfc3c9b679b0","value":" 187/187 [00:00&lt;00:00, 24.7kB/s]"}},"8c8cc35b95214f5ab7659e656d9b6134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da809853b1640439e941358a0f38d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e283e09688e4febb699f4789be996b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"941aa19f00454faf9c0673532fed042f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef0eae54863f4af5b978e2b40bc44886":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7742b64b7f064344bdcabbb73c4bc361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cedd1ba46d3949eda270cfc3c9b679b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}